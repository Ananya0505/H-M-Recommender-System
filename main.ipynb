{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;font-size:150%;text-align:center;border-radius:10px 10px;border-style:solid;border-color:#d90b1c;\">Recommendation system for H and M Fashion</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">Terminologies</h1>\n",
    "\n",
    "There are certain terminologies which needs to be understood before moving forward.\n",
    "\n",
    "**Apache Spark:** Apache Spark is an open-source distributed general-purpose cluster-computing framework.It can be used with Hadoop too.\n",
    "\n",
    "**Collaborative filtering:** Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users. Consider example if a person A likes item 1, 2, 3 and B like 2,3,4 then they have similar interests and A should like item 4 and B should like item 1.\n",
    "\n",
    "**Alternating least square(ALS) matrix factorization:** The idea is basically to take a large (or potentially huge) matrix and factor it into some smaller representation of the original matrix through alternating least squares. We end up with two or more lower dimensional matrices whose product equals the original one.ALS comes inbuilt in Apache Spark.\n",
    "\n",
    "**PySpark:** PySpark is the collaboration of Apache Spark and Python. PySpark is the Python API for Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">1.Initialize spark session</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-07T01:17:35.342878Z",
     "iopub.status.busy": "2023-05-07T01:17:35.342571Z",
     "iopub.status.idle": "2023-05-07T01:18:18.333605Z",
     "shell.execute_reply": "2023-05-07T01:18:18.332678Z",
     "shell.execute_reply.started": "2023-05-07T01:17:35.342799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
      "     |████████████████████████████████| 310.8 MB 8.7 kB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     |████████████████████████████████| 200 kB 53.3 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317143 sha256=1c6ec3dd91c997b6a8091cba738aed3153194ec170e8bf6e6a4edd36ba7f0adc\n",
      "  Stored in directory: /root/.cache/pip/wheels/06/51/98/f7a41aad64c08302d6c26c90650e713c3dfeb5cdec4946db00\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.3\n",
      "    Uninstalling py4j-0.10.9.3:\n",
      "      Successfully uninstalled py4j-0.10.9.3\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">2-Load libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-05-07T01:18:28.337571Z",
     "iopub.status.busy": "2023-05-07T01:18:28.337285Z",
     "iopub.status.idle": "2023-05-07T01:18:34.122354Z",
     "shell.execute_reply": "2023-05-07T01:18:34.121489Z",
     "shell.execute_reply.started": "2023-05-07T01:18:28.337538Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/07 01:18:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/opt/conda/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import col,array_contains\n",
    "from pyspark.sql import SQLContext \n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import udf,col,when\n",
    "from pyspark.sql.functions import to_timestamp,date_format\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "sc = SparkSession.builder.appName(\"Recommendations\").config(\"spark.sql.files.maxPartitionBytes\", 5000000).getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">3-Load Dataset in Apache Spark</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T01:18:39.165384Z",
     "iopub.status.busy": "2023-05-07T01:18:39.165086Z",
     "iopub.status.idle": "2023-05-07T01:18:45.548156Z",
     "shell.execute_reply": "2023-05-07T01:18:45.547401Z",
     "shell.execute_reply.started": "2023-05-07T01:18:39.165351Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- t_dat: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- article_id: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- sales_channel_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction = spark.read.option(\"header\",True) \\\n",
    "              .csv(\"../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv\")\n",
    "transaction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T01:18:59.905478Z",
     "iopub.status.busy": "2023-05-07T01:18:59.905213Z",
     "iopub.status.idle": "2023-05-07T01:20:01.615763Z",
     "shell.execute_reply": "2023-05-07T01:20:01.615025Z",
     "shell.execute_reply.started": "2023-05-07T01:18:59.905448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2018-09-20', '2020-09-22')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "from pyspark.sql.functions import unix_timestamp, lit\n",
    "min_date, max_date = transaction.select(min(\"t_dat\"), max(\"t_dat\")).first()\n",
    "min_date, max_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">5-Select data for recommendation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this transaction dataset we have 31,788,324 rows and 5 columns.Let's capture first what are the most recently bought articles.For recommendation I am selecting only date 2020-09-22 which is the last transaction date.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T01:20:15.465175Z",
     "iopub.status.busy": "2023-05-07T01:20:15.464897Z",
     "iopub.status.idle": "2023-05-07T01:22:00.248764Z",
     "shell.execute_reply": "2023-05-07T01:22:00.248042Z",
     "shell.execute_reply.started": "2023-05-07T01:20:15.465145Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:======================================================>(696 + 2) / 698]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|         customer_id|article_id|count|\n",
      "+--------------------+----------+-----+\n",
      "|00f7bc5c0df4c615b...|0780418013|    1|\n",
      "|02094817e46f3b692...|0791587001|    1|\n",
      "|0333e5dda0257e9f4...|0839332002|    2|\n",
      "|07c7a1172caf8fb97...|0573085043|    1|\n",
      "|081373184e601470c...|0714790020|    1|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "hm =  transaction.withColumn('t_dat', transaction['t_dat'].cast('string'))\n",
    "hm = hm.withColumn('date', from_unixtime(unix_timestamp('t_dat', 'yyyy-MM-dd')))\n",
    "hm = hm.withColumn('year', year(col('date')))\n",
    "hm = hm.withColumn('month', month(col('date')))\n",
    "hm = hm.withColumn('day', date_format(col('date'), \"d\"))\n",
    "\n",
    "hm = hm[hm['year'] == 2020]\n",
    "hm = hm[hm['month'] == 9]\n",
    "hm = hm[hm['day'] == 22]\n",
    "transaction.unpersist()\n",
    "\n",
    "# Prepare the dataset\n",
    "hm = hm.groupby('customer_id', 'article_id').count()\n",
    "hm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T01:22:06.097259Z",
     "iopub.status.busy": "2023-05-07T01:22:06.096952Z",
     "iopub.status.idle": "2023-05-07T01:23:46.842110Z",
     "shell.execute_reply": "2023-05-07T01:23:46.841446Z",
     "shell.execute_reply.started": "2023-05-07T01:22:06.097225Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:======================================================>(697 + 1) / 698]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29486, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((hm.count(), len(hm.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T01:24:08.626330Z",
     "iopub.status.busy": "2023-05-07T01:24:08.626065Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=============>                                        (176 + 2) / 698]\r"
     ]
    }
   ],
   "source": [
    "# Count the total number of article in the dataset\n",
    "numerator = hm.select(\"count\").count()\n",
    "\n",
    "# Count the number of distinct customerid and distinct articleid\n",
    "num_users = hm.select(\"customer_id\").distinct().count()\n",
    "num_articles = hm.select(\"article_id\").distinct().count()\n",
    "\n",
    "# Set the denominator equal to the number of customer multiplied by the number of articles\n",
    "denominator = num_users * num_articles\n",
    "\n",
    "# Divide the numerator by the denominator\n",
    "sparsity = (1.0 - (numerator *1.0)/denominator)*100\n",
    "print(\"Sparsity: \", \"%.2f\" % sparsity + \"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T17:52:01.21543Z",
     "iopub.status.busy": "2022-04-29T17:52:01.21517Z",
     "iopub.status.idle": "2022-04-29T17:53:56.456318Z",
     "shell.execute_reply": "2022-04-29T17:53:56.455225Z",
     "shell.execute_reply.started": "2022-04-29T17:52:01.215399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "userId_count = hm.groupBy(\"customer_id\").count().orderBy('count', ascending=False)\n",
    "userId_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T17:54:05.593265Z",
     "iopub.status.busy": "2022-04-29T17:54:05.593015Z",
     "iopub.status.idle": "2022-04-29T17:56:03.745179Z",
     "shell.execute_reply": "2022-04-29T17:56:03.74449Z",
     "shell.execute_reply.started": "2022-04-29T17:54:05.593236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "articleId_count = hm.groupBy(\"article_id\").count().orderBy('count', ascending=False)\n",
    "articleId_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">5-Importing important modules</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T17:56:12.98482Z",
     "iopub.status.busy": "2022-04-29T17:56:12.984187Z",
     "iopub.status.idle": "2022-04-29T17:56:12.989312Z",
     "shell.execute_reply": "2022-04-29T17:56:12.988317Z",
     "shell.execute_reply.started": "2022-04-29T17:56:12.984779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">6-Converting String to index</h1>\n",
    "\n",
    "Before making an ALS model it needs to be clear that ALS only accepts integer value as parameters. Hence we need to convert customer_id and article_id column in index form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T17:56:22.00728Z",
     "iopub.status.busy": "2022-04-29T17:56:22.006835Z",
     "iopub.status.idle": "2022-04-29T18:03:06.47884Z",
     "shell.execute_reply": "2022-04-29T18:03:06.478177Z",
     "shell.execute_reply.started": "2022-04-29T17:56:22.007241Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "indexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in list(set(hm.columns)-set(['count'])) ]\n",
    "pipeline = Pipeline(stages=indexer)\n",
    "transformed = pipeline.fit(hm).transform(hm)\n",
    "transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">7-Creating training and test data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T18:03:22.490053Z",
     "iopub.status.busy": "2022-04-29T18:03:22.489687Z",
     "iopub.status.idle": "2022-04-29T18:03:22.51593Z",
     "shell.execute_reply": "2022-04-29T18:03:22.515163Z",
     "shell.execute_reply.started": "2022-04-29T18:03:22.490017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "(training,test)=transformed.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">8-Creating ALS model and fitting data</h1>\n",
    "\n",
    "To build the model explicitly specify the columns. Set nonnegative as ‘True’, since we are looking count greater than 0. The model also gives an option to select implicit ratings. Since we are working with explicit, set it to ‘False’ or by default it takes explicit.\n",
    "\n",
    "When using simple random splits as in Spark’s CrossValidator or TrainValidationSplit, it is actually very common to encounter users and/or items in the evaluation set that are not in the training set. By default, Spark assigns NaN predictions during ALSModel.transform when a user and/or item factor is not present in the model.We set cold start strategy to ‘drop’ to ensure we don’t get NaN evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-04-29T18:03:29.61416Z",
     "iopub.status.busy": "2022-04-29T18:03:29.613902Z",
     "iopub.status.idle": "2022-04-29T19:05:05.712355Z",
     "shell.execute_reply": "2022-04-29T19:05:05.711632Z",
     "shell.execute_reply.started": "2022-04-29T18:03:29.614132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "#create ALS model\n",
    "als=ALS(userCol=\"customer_id_index\",itemCol=\"article_id_index\",ratingCol=\"count\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "\n",
    "#tune model using ParamGridBuilder\n",
    "param_grid = ParamGridBuilder()\\\n",
    "            .addGrid(als.rank, [15,20,25])\\\n",
    "            .addGrid(als.maxIter,[5,10,15])\\\n",
    "            .addGrid(als.regParam,[0.09,0.14,0.19])\\\n",
    "            .build()\n",
    "#define evaluator as RMSE\n",
    "evaluator = RegressionEvaluator(metricName = \"rmse\",labelCol = 'count', predictionCol = 'prediction')\n",
    "\n",
    "#Build cross validation using CrossValidator\n",
    "cv = CrossValidator(estimator=als,estimatorParamMaps=param_grid, evaluator=evaluator,numFolds=3)\n",
    "\n",
    "\n",
    "#Fit ALS model to training data\n",
    "model = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-04-25T10:33:54.896652Z",
     "iopub.status.busy": "2022-04-25T10:33:54.896318Z",
     "iopub.status.idle": "2022-04-25T10:37:58.043826Z",
     "shell.execute_reply": "2022-04-25T10:37:58.039774Z",
     "shell.execute_reply.started": "2022-04-25T10:33:54.896611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"als=ALS(maxIter=5,regParam=0.09,rank=25,userCol=\"customer_id_index\",itemCol=\"article_id_index\",ratingCol=\"count\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "model=als.fit(training)\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">9-Evaluate rmse</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-04-29T19:06:21.317622Z",
     "iopub.status.busy": "2022-04-29T19:06:21.317333Z",
     "iopub.status.idle": "2022-04-29T19:08:18.947865Z",
     "shell.execute_reply": "2022-04-29T19:08:18.947078Z",
     "shell.execute_reply.started": "2022-04-29T19:06:21.317592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Extract best model from the tuning exercise using ParamGridBuilder\n",
    "best_model = model.bestModel\n",
    "\n",
    "#Generate predictions and evaluate using RMSE\n",
    "predictions = best_model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T19:08:23.274555Z",
     "iopub.status.busy": "2022-04-29T19:08:23.274279Z",
     "iopub.status.idle": "2022-04-29T19:08:23.283855Z",
     "shell.execute_reply": "2022-04-29T19:08:23.28299Z",
     "shell.execute_reply.started": "2022-04-29T19:08:23.274523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#print evaluation metrics and model parameters\n",
    "print(\"RMSE =\" + str(rmse))\n",
    "print(\"**Best Model**\")\n",
    "print(\"Rank : {}\".format(best_model.rank))\n",
    "print(\"MaxIter: {}\".format(best_model._java_obj.parent().getMaxIter()))\n",
    "print(\"RegParam: {}\".format(best_model._java_obj.parent().getRegParam()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">10-Providing Recommendations by Article id</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T19:08:42.261749Z",
     "iopub.status.busy": "2022-04-29T19:08:42.261496Z",
     "iopub.status.idle": "2022-04-29T19:08:54.227188Z",
     "shell.execute_reply": "2022-04-29T19:08:54.226528Z",
     "shell.execute_reply.started": "2022-04-29T19:08:42.261721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_recs=best_model.recommendForAllItems(10).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">11-Providing Recommendations by Customer id</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T19:16:38.06909Z",
     "iopub.status.busy": "2022-04-29T19:16:38.068545Z",
     "iopub.status.idle": "2022-04-29T19:16:50.265156Z",
     "shell.execute_reply": "2022-04-29T19:16:50.263935Z",
     "shell.execute_reply.started": "2022-04-29T19:16:38.069048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_recom = best_model.recommendForAllUsers(10)\n",
    "df_recom.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T19:17:03.941344Z",
     "iopub.status.busy": "2022-04-29T19:17:03.941087Z",
     "iopub.status.idle": "2022-04-29T19:17:26.320994Z",
     "shell.execute_reply": "2022-04-29T19:17:26.32033Z",
     "shell.execute_reply.started": "2022-04-29T19:17:03.941312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_recom = df_recom.select(\"customer_id_index\",\"recommendations.article_id_index\")\n",
    "df_recom.show(10)\n",
    "df_recom = df_recom.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T19:17:38.278501Z",
     "iopub.status.busy": "2022-04-29T19:17:38.278234Z",
     "iopub.status.idle": "2022-04-29T19:17:38.29858Z",
     "shell.execute_reply": "2022-04-29T19:17:38.297941Z",
     "shell.execute_reply.started": "2022-04-29T19:17:38.278454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_recom.sort_values('customer_id_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">12-Converting back to string form</h1>\n",
    "\n",
    "As seen in above image the results are in integer form we need to convert it back to its original name.The code is little bit longer given so many conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T19:17:48.043736Z",
     "iopub.status.busy": "2022-04-29T19:17:48.043068Z",
     "iopub.status.idle": "2022-04-29T19:19:52.557095Z",
     "shell.execute_reply": "2022-04-29T19:19:52.556348Z",
     "shell.execute_reply.started": "2022-04-29T19:17:48.043695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "md=transformed.select(transformed['article_id'],transformed['article_id_index'],transformed['customer_id'],transformed['customer_id_index'])\n",
    "md=md.toPandas()\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T19:20:04.380447Z",
     "iopub.status.busy": "2022-04-29T19:20:04.380198Z",
     "iopub.status.idle": "2022-04-29T19:20:04.462673Z",
     "shell.execute_reply": "2022-04-29T19:20:04.461867Z",
     "shell.execute_reply.started": "2022-04-29T19:20:04.380419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dict1 =dict(zip(md['article_id_index'],md['article_id']))\n",
    "dict2=dict(zip(md['customer_id_index'],md['customer_id']))\n",
    "df_recom['article_id'] = df_recom['article_id_index'].map(lambda x: [dict1[y] for y in x if y in dict1])\n",
    "df_recom['customer_id']=df_recom['customer_id_index'].map(dict2)\n",
    "df_recom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T19:27:03.683519Z",
     "iopub.status.busy": "2022-04-29T19:27:03.683254Z",
     "iopub.status.idle": "2022-04-29T19:27:03.70264Z",
     "shell.execute_reply": "2022-04-29T19:27:03.70193Z",
     "shell.execute_reply.started": "2022-04-29T19:27:03.68347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "recom_final = df_recom.drop(['customer_id_index','article_id_index'], axis = 1)\n",
    "finalpre=recom_final[['customer_id','article_id']]\n",
    "finalpre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#f7e9ec;font-family:newtimeroman;color:#d90b1c;\">13-Export the prediction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T19:27:16.893949Z",
     "iopub.status.busy": "2022-04-29T19:27:16.893422Z",
     "iopub.status.idle": "2022-04-29T19:27:16.911154Z",
     "shell.execute_reply": "2022-04-29T19:27:16.910344Z",
     "shell.execute_reply.started": "2022-04-29T19:27:16.893912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "my_pred = finalpre.toPandas()\n",
    "my_pred.to_csv('my_pred.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
